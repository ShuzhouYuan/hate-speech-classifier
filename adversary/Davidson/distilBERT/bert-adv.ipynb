{"cells":[{"metadata":{"id":"lLGJyJthW1F-","trusted":true,"outputId":"bfd159ba-fcac-4668-8fd6-ba50f5b13f5e"},"cell_type":"code","source":"!pip install tokenizers==0.9.4\n!pip install simpletransformers","execution_count":null,"outputs":[]},{"metadata":{"id":"G9oN1CmeAJIq","trusted":true},"cell_type":"code","source":"from transformers import DistilBertTokenizer, AdamW, DistilBertModel, DistilBertConfig\nimport pandas as pd\nimport torch\nfrom torch.utils.data import TensorDataset, random_split\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nimport time\nimport torch.nn as nn","execution_count":1,"outputs":[]},{"metadata":{"id":"LbVh7zgKnJRL","trusted":true,"outputId":"ec8cbe8a-e294-4847-cf40-fb09a1dacea3"},"cell_type":"code","source":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(DEVICE)","execution_count":2,"outputs":[{"output_type":"stream","text":"cuda\n","name":"stdout"}]},{"metadata":{"id":"QCSs4ei1Xor8"},"cell_type":"markdown","source":"## Read data from csv files"},{"metadata":{"id":"tLEIXzsi07C8","trusted":true},"cell_type":"code","source":"def read_train_and_test(train_path, test_path): #path: path of the whole dataset\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n    return train, test\n\ntrain_path = \"../input/re-balance-train/re_balance_train2.csv\"\ntest_path = \"../input/raw-train-test/test.csv\"\ntrain_csv, test_csv = read_train_and_test(train_path, test_path)","execution_count":3,"outputs":[]},{"metadata":{"id":"y_4ZHk66VAA6","trusted":true,"outputId":"e2289aac-c523-4107-bf83-f9a831bf55b7"},"cell_type":"code","source":"train_labels = list(train_csv.label.values)\nprint(train_labels.count(0))\nprint(train_labels.count(1))\nprint(train_labels.count(2))","execution_count":4,"outputs":[{"output_type":"stream","text":"1136\n1136\n1136\n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"36MMpQTxMCaA","outputId":"45ad5a92-9cf7-42b3-86dc-d53786eec0b1"},"cell_type":"code","source":"print(train_csv.tweet.values[:10])","execution_count":5,"outputs":[{"output_type":"stream","text":"['@LandonVanBus Charlie Day?'\n \"omg RT @SaddyBey: Fat bitch. What's her @? http://t.co/ptNszx6nid\"\n 'RT @LizardLickTowin: - @CMchatLIVE #CMchat why do U think its so hard for\"hick hop\"2get major air time on most stations #cmchat when it\\'s w&#8230;'\n 'RT @CHlLDHOODRUINER: \"Great.... DaQuan got her pregnant and now I\\'m gonna have a nigger grandbaby\" http://t.co/9aAyrrKNbt'\n \"Trust none bitch I don't wife a bitch? Bitch I won't\"\n 'RT @ItsFoodPorn: Oreo cheesecake bites! http://t.co/iHgu1ZuyZt'\n 'DIVERSITY: Chcgo Blck Pstr Flooded w/ Dth Threats Ovr Endorsement Of GOP Candidate: &#8220;U Sellout Uncle Tom Ass N*gger&#8221; http://t.co/vcBWzkyeDv'\n '\"All these bitches want a baby, I don\\'t want no children.\"'\n 'I know Outback when I see Outback, hoe.'\n 'RT @SteveStfler: ur a faggot if you change your name to your bday just so you can get bday tweets. this aint FB nigga, fuck yo birthday']\n","name":"stdout"}]},{"metadata":{"id":"cyRsPqwLX1wy"},"cell_type":"markdown","source":"## Run tokenizer"},{"metadata":{"id":"DvIQKI-AtzJY","trusted":true,"outputId":"94d4bd81-f261-428c-a9c4-e9b80455effd"},"cell_type":"code","source":"def run_tokenizer(train_csv, test_csv, merge_label=False, add_token=False):\n    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased') \n    if add_token:\n      tokenizer.add_tokens(add_token)\n\n    def get_max_len(tokenizer, train_csv):\n        tweets = train_csv.tweet.values\n        max_length = 0\n        for t in tweets:\n          ids = tokenizer.encode(t)\n          max_length = max(len(ids),max_length)\n        return max_length\n\n    max_length = get_max_len(tokenizer, train_csv)\n    train_tweets, train_labels = train_csv.tweet.values, train_csv.label.values\n    test_tweets, test_labels = test_csv.tweet.values, test_csv.label.values\n    if merge_label == True:\n       train_labels = [l if l ==0 else 1 for l in train_labels]\n       test_labels = [l if l ==0 else 1 for l in test_labels]\n\n    def tokenize_for_tweet(tokenizer, tweets, labels):\n        input_ids = []\n        attention_masks = []\n\n        for t in tweets:\n            input_dict = tokenizer.encode_plus(t, add_special_tokens=True, max_length=max_length, truncation=True, padding='max_length',return_tensors='pt')\n            input_ids.append(input_dict['input_ids'])\n            attention_masks.append(input_dict['attention_mask'])\n        input_ids = torch.cat(input_ids,dim=0)\n        attention_masks = torch.cat(attention_masks,dim=0)\n        labels=torch.tensor(labels)\n        dataset = TensorDataset(input_ids, attention_masks, labels)\n        return dataset\n        \n    train_dataset = tokenize_for_tweet(tokenizer, train_tweets, train_labels)\n    test_dataset = tokenize_for_tweet(tokenizer, test_tweets, test_labels)\n    num_label = 3 if merge_label == False else 2\n    return train_dataset, test_dataset, num_label, tokenizer\n\n#tokens = ['<SWEAR-0>', '<SWEAR-1>', '<SWEAR-2>', '<SWEAR-3>', '<SWEAR-4>']\ntrain_dataset, test_dataset, num_label, tokenizer = run_tokenizer(train_csv, test_csv, merge_label = False)#,add_token = tokens)","execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89de23bbac1d479796da6d635c39ee83"}},"metadata":{}}]},{"metadata":{"id":"nE6x0jARX7hy"},"cell_type":"markdown","source":"## Make data loader"},{"metadata":{"id":"mAfIYownmjsZ","trusted":true},"cell_type":"code","source":"batch_size = 20\n\ntrain_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset), batch_size = batch_size)\n\ntest_dataloader = DataLoader(test_dataset, sampler = SequentialSampler(test_dataset), batch_size = batch_size)","execution_count":7,"outputs":[]},{"metadata":{"id":"zcNUTLkIYAnd"},"cell_type":"markdown","source":"## Define classifier and adversary"},{"metadata":{"id":"b18YlcZR81n5","trusted":true},"cell_type":"code","source":"configuration = DistilBertConfig()\n#configuration.output_hidden_states = True\n#print(configuration)","execution_count":8,"outputs":[]},{"metadata":{"id":"Foric77dco5V","trusted":true},"cell_type":"code","source":"class Classifier(nn.Module):\n  def __init__(self, num_label):\n    super().__init__()\n    self.bert = DistilBertModel.from_pretrained('distilbert-base-cased')\n    #self.bert.resize_token_embeddings(len(tokenizer))\n    self.linear = nn.Linear(configuration.hidden_size, num_label)\n\n  def forward(self, input_ids, attention_mask): # input_id [batch_size, sentence_length]\n    last_hidden_state = self.bert(input_ids, attention_mask)[0] # last_hidden_state [batch_size, sentence_length, hidden_size]\n    last_hidden_state_mean = torch.mean(last_hidden_state, dim=1) # last_hidden_state [batch_size, hidden_size]\n    output = self.linear(last_hidden_state_mean) # output [batch_size, num_label]\n    return last_hidden_state_mean, output\n","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Adversary_lstm(nn.Module):\n  def __init__(self, num_protected_label, hidden_size):\n    super().__init__()\n    self.lstm = nn.LSTM(configuration.hidden_size, hidden_size,bidirectional=True, batch_first=True)\n    #self.relu = nn.ReLU()\n    self.linear = nn.Linear(hidden_size*2, num_protected_label)\n\n  def forward(self, clf_input):\n    #clf_input = torch.unsqueeze(clf_input, dim=1)\n    lstm, _ = self.lstm(clf_input)\n    #lstm = torch.squeeze(lstm, dim=1)\n    lstm = lstm[:,-1,:]\n    #output1 = self.relu(lstm)\n    output = self.linear(lstm)\n    return output","execution_count":null,"outputs":[]},{"metadata":{"id":"RRSb2kh4_9YB","trusted":true},"cell_type":"code","source":"class Adversary(nn.Module):\n  def __init__(self, num_protected_label, hidden_size):\n    super().__init__()\n    self.linear1 = nn.Linear(configuration.hidden_size, hidden_size)\n    self.relu = nn.ReLU()\n    self.linear2 = nn.Linear(hidden_size, num_protected_label)\n\n  def forward(self, clf_last_state):\n    output1 = self.relu(self.linear1(clf_last_state))\n    output = self.linear2(output1)\n    return output","execution_count":10,"outputs":[]},{"metadata":{"id":"bDBVcb9HYIRg"},"cell_type":"markdown","source":"## Import some tools for evaluation after each training epoch"},{"metadata":{"trusted":true,"id":"WpoTduN5MCaE"},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import classification_report\nimport numpy as np\nimport random","execution_count":11,"outputs":[]},{"metadata":{"trusted":true,"id":"kkQQj3zDMCaE"},"cell_type":"code","source":"def plot_confusion_matrix(y_true, y_pred, classes, normalize=False, title=None, cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    #classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax","execution_count":12,"outputs":[]},{"metadata":{"trusted":true,"id":"aFFmYy2aMCaE"},"cell_type":"code","source":"def print_matrix(true_labels, predict_labels, default_classes=['hate','offensive','neither']):\n  cm = confusion_matrix(true_labels, predict_labels)\n  plot_confusion_matrix(true_labels, predict_labels, classes=default_classes ,title='Confusion matrix with rebalance')\n\n  #Plot normalized confusion matrix\n  plot_confusion_matrix(true_labels, predict_labels, classes=default_classes, normalize=True,title='Normalized confusion matrix with rebalance')\n\n  plt.show()\n\n  print(classification_report(true_labels, predict_labels, target_names=default_classes))","execution_count":13,"outputs":[]},{"metadata":{"id":"UKmFT1zKYQJ5"},"cell_type":"markdown","source":"## Training and evaluation function"},{"metadata":{"id":"laM3Y-4qFMRN","trusted":true},"cell_type":"code","source":"def joint_training(clf, adv, epochs, clf_optimizer, adv_optimizer, train_dataloader, test_dataloader, alpha, model_name):\n    best_acc = 0\n    for e in range(epochs):\n        print('training {} epoch...'.format(e+1))\n        start_time = time.time()\n\n        train_loss, total_clf_loss, total_adv_loss = 0, 0, 0\n\n        clf.train(True)\n        adv.train(True)\n        for input, mask, label in train_dataloader:\n            input = input.to(DEVICE)\n            mask = mask.to(DEVICE)\n            label=label.to(DEVICE)\n\n            protected_label = torch.tensor([1 if l == 1 else 0 for l in label], dtype=torch.long).to(DEVICE) # 0 no-offensive 1 offensive\n            \n            clf.zero_grad()\n            adv.zero_grad()\n\n            last_hidden_state, clf_output = clf(input_ids=input, attention_mask=mask)\n\n            adv_output = adv(last_hidden_state)\n\n            clf_loss = loss_function(clf_output, label)\n            adv_loss = loss_function(adv_output, protected_label)\n\n            total_loss = clf_loss + alpha*adv_loss\n\n            train_loss += total_loss.item()\n            total_clf_loss += clf_loss.item()\n            total_adv_loss += adv_loss.item()\n\n            total_loss.backward(retain_graph=True)\n            clf_optimizer.step()\n\n            adv_loss.backward()\n            adv_optimizer.step()\n            \n\n        avg_train_loss = train_loss / len(train_dataloader)\n        sec = time.time()-start_time\n        print('{} seconds used......'.format(sec))\n        print(\"{} training finished! average train loss: {}\".format(e+1,avg_train_loss))\n        print('total clf loss: {} total adv loss: {}'.format(total_clf_loss, total_adv_loss))\n        print('evaluating...')\n        best_acc = evaluate(clf, best_acc, test_dataloader, model_name, str(e+1))\n        #print('evaluating adversary...')\n        #evaluate_adv(adv, clf, test_dataloader)\n        #torch.save(adv,\"adv\"+str(e+1))\n        ","execution_count":14,"outputs":[]},{"metadata":{"id":"5FAxcsnLHYWp","trusted":true},"cell_type":"code","source":"def evaluate(clf, best_acc, test_dataloader, model_name, epoch):\n    num_total, num_correct = 0, 0\n    clf.train(False)\n    with torch.no_grad():\n      eval_loss = 0\n      true_labels, predict_labels = [], []\n      for input, mask, label in test_dataloader:\n          clf.zero_grad()\n          \n          input = input.to(DEVICE)\n          mask = mask.to(DEVICE)\n          label = label.to(DEVICE)\n\n          last_hidden_state, output = clf(input_ids=input, attention_mask=mask)\n\n          loss = loss_function(output, label)\n\n          predict_label = torch.argmax(output, dim=1)\n\n          true_labels += label.tolist()\n          predict_labels += predict_label.tolist()\n\n          num_correct += (predict_label == label).sum().item()\n          num_total += len(label)\n\n          eval_loss += loss.item()\n\n      avg_eval_loss = eval_loss / len(test_dataloader)\n\n      acc = num_correct/num_total\n      if acc > best_acc:\n        best_acc = acc\n      torch.save(clf, epoch+model_name)\n      print_matrix(true_labels, predict_labels)\n\n    print('average eval_loss: {}, accuracy: {}'.format(avg_eval_loss,acc))\n    return best_acc\n","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_adv(adv, clf, test_dataloader):\n    clf.train(False)\n    adv.train(False)\n    with torch.no_grad():\n        true_labels, predict_labels = [], []\n        for input, mask, label in test_dataloader:\n            input = input.to(DEVICE)\n            mask = mask.to(DEVICE)\n            label=label.to(DEVICE)\n\n            protected_label = torch.tensor([1 if l == 1 else 0 for l in label], dtype=torch.long).to(DEVICE) # 0 no-offensive 1 offensive\n            \n            clf.zero_grad()\n            adv.zero_grad()\n\n            last_hidden_state, clf_output = clf(input_ids=input, attention_mask=mask)\n\n            adv_output = adv(last_hidden_state)\n\n            predict_label = torch.argmax(adv_output, dim=1)\n            true_labels += protected_label.tolist()\n            predict_labels += predict_label.tolist()\n        print_matrix(true_labels, predict_labels, default_classes=['no offensive','offensive']) \n             ","execution_count":null,"outputs":[]},{"metadata":{"id":"hV2RTigKYfSu"},"cell_type":"markdown","source":"## Define the classifier and adversary"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_clf(clf, epochs, clf_optimizer, train_dataloader, test_dataloader, model_name):\n    best_acc = 0\n    for e in range(epochs):\n        print('training {} epoch...'.format(e+1))\n        start_time = time.time()\n\n        train_loss = 0\n\n        clf.train(True)\n        for input, mask, label in train_dataloader:\n            input = input.to(DEVICE)\n            mask = mask.to(DEVICE)\n            label=label.to(DEVICE)\n            \n            clf.zero_grad()\n\n            last_hidden_state, clf_output = clf(input_ids=input, attention_mask=mask)\n                       \n            clf_loss = loss_function(clf_output, label)\n            \n            train_loss += clf_loss.item()\n\n            clf_loss.backward()\n            clf_optimizer.step()\n\n        avg_train_loss = train_loss / len(train_dataloader)\n        sec = time.time()-start_time\n        print('{} seconds used......'.format(sec))\n        print(\"{} training finished! average train loss: {}\".format(e+1,avg_train_loss))\n        print('evaluating...')\n        best_acc = evaluate(clf, best_acc, test_dataloader, model_name, str(e+1))","execution_count":16,"outputs":[]},{"metadata":{"id":"hIWH9p-8_Gz4","outputId":"80f69313-67a9-4dc0-a442-8534312fc81d","trusted":true},"cell_type":"code","source":"classifier = Classifier(num_label).to(DEVICE)\n\n#classifier = torch.load('./1distilbert_clf')\n\nadversary = Adversary(2, 200).to(DEVICE)\n\nloss_function = nn.CrossEntropyLoss()\n\nclf_optimizer = AdamW(classifier.parameters(),lr = 2e-5, eps = 1e-8)\n\nadv_optimizer = torch.optim.AdamW(adversary.parameters(), lr=0.001)","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_clf(classifier, 10, clf_optimizer, train_dataloader, test_dataloader, 'distilbert_clf')","execution_count":null,"outputs":[{"output_type":"stream","text":"training 1 epoch...\n","name":"stdout"}]},{"metadata":{"id":"QUsp9GmZZQZ2"},"cell_type":"markdown","source":"## Joint training\nIn this step, only the total loss which is defined as L = L<sub>clf</sub> − α∗L<sub>adv</sub> is used for backpropagation. Hence, α is also a import hyperparameter. "},{"metadata":{"id":"SoXzLzmgKP8Z","trusted":true,"outputId":"7416bfd9-639a-469c-b3e3-f450c84dc111"},"cell_type":"code","source":"epochs = 10\nalpha = 2\nmodel_name = 'adv_hate_as_noffensive'\n\njoint_training(classifier, adversary, epochs, clf_optimizer, adv_optimizer, train_dataloader, test_dataloader, alpha, model_name)","execution_count":null,"outputs":[]},{"metadata":{"id":"Ef7rhBsgaWbw"},"cell_type":"markdown","source":"## Rebalance the number of instances in classes\nIn the prevous experiment, rebalancing is an effective method to increase the true positive rate for hate speech class. (However, the scores of other classes will decrease)"},{"metadata":{"trusted":true,"id":"sCWBAqc4MCaF"},"cell_type":"code","source":"def rebalance_dataset(dataset):\n    data_0 = [inst for inst in dataset if inst[2]==0]\n    data_1 = [inst for inst in dataset if inst[2]==1]\n    data_2 = [inst for inst in dataset if inst[2]==2]\n    random.shuffle(data_0)\n    random.shuffle(data_1)\n    random.shuffle(data_2)\n    num_data0 = len(data_0)\n    balanced_dataset = data_0+data_1[:num_data0]+data_2[:num_data0]\n    return balanced_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"Sa8xJ7wRMCaF"},"cell_type":"code","source":"re_train_dataset = rebalance_dataset(train_dataset)\nre_test_dataset = rebalance_dataset(test_dataset)","execution_count":null,"outputs":[]},{"metadata":{"id":"lLoCX_30dm9m","trusted":true},"cell_type":"code","source":"batch_size = 20","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"I66AVk9iMCaF"},"cell_type":"code","source":"re_train_dataloader = DataLoader(re_train_dataset, sampler = RandomSampler(re_train_dataset), batch_size = batch_size)\nre_test_dataloader = DataLoader(re_test_dataset, sampler = RandomSampler(re_test_dataset), batch_size = batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"HzXhQc6HMCaF"},"cell_type":"code","source":"epochs = 10\nalpha = 2\nmodel_name = 'adv_model_hate_as_noffensive'\njoint_training(classifier, adversary, epochs, clf_optimizer, adv_optimizer, re_train_dataloader, test_dataloader, alpha, model_name)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}