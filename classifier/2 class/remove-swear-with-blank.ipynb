{"cells":[{"metadata":{"executionInfo":{"elapsed":6889,"status":"ok","timestamp":1615028072702,"user":{"displayName":"Schlözer","photoUrl":"","userId":"15113573867308042460"},"user_tz":-60},"id":"lLGJyJthW1F-","outputId":"3a4eb2ed-df87-433c-d6e5-47d82dbc716d","trusted":true},"cell_type":"code","source":"!pip install transformers==4.2.0\n#!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp36-cp36m-linux_x86_64.whl","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting transformers==4.2.0\n  Downloading transformers-4.2.0-py3-none-any.whl (1.8 MB)\n\u001b[K     |████████████████████████████████| 1.8 MB 1.3 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.2.0) (2020.11.13)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers==4.2.0) (1.19.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.2.0) (4.55.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.2.0) (2.25.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.2.0) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==4.2.0) (20.8)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.2.0) (0.0.43)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.2.0) (3.0.12)\nRequirement already satisfied: tokenizers==0.9.4 in /opt/conda/lib/python3.7/site-packages (from transformers==4.2.0) (0.9.4)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.2.0) (3.4.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.2.0) (3.7.4.3)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==4.2.0) (2.4.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.2.0) (2020.12.5)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.2.0) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.2.0) (1.26.2)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.2.0) (3.0.4)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.2.0) (7.1.2)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.2.0) (1.0.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.2.0) (1.15.0)\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.2.2\n    Uninstalling transformers-4.2.2:\n      Successfully uninstalled transformers-4.2.2\nSuccessfully installed transformers-4.2.0\n","name":"stdout"}]},{"metadata":{"executionInfo":{"elapsed":6906,"status":"ok","timestamp":1615028076949,"user":{"displayName":"Schlözer","photoUrl":"","userId":"15113573867308042460"},"user_tz":-60},"id":"G9oN1CmeAJIq","trusted":true},"cell_type":"code","source":"from transformers import BertForSequenceClassification, DistilBertTokenizerFast, AdamW, get_linear_schedule_with_warmup\nimport pandas as pd\nimport torch\nfrom torch.utils.data import TensorDataset, random_split\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nimport time\nimport pickle","execution_count":2,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":3093,"status":"ok","timestamp":1615028076949,"user":{"displayName":"Schlözer","photoUrl":"","userId":"15113573867308042460"},"user_tz":-60},"id":"LbVh7zgKnJRL","outputId":"b73cbc8a-5815-4185-8fd5-38d502be1ba4","trusted":true},"cell_type":"code","source":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(DEVICE)","execution_count":3,"outputs":[{"output_type":"stream","text":"cuda\n","name":"stdout"}]},{"metadata":{"executionInfo":{"elapsed":1059,"status":"ok","timestamp":1615028078848,"user":{"displayName":"Schlözer","photoUrl":"","userId":"15113573867308042460"},"user_tz":-60},"id":"tLEIXzsi07C8","trusted":true},"cell_type":"code","source":"def read_train_and_test(train_path, test_path): #path: path of the whole dataset\n  #try:\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n  #except FileNotFoundError:\n    #data_csv = pd.read_csv(path, names=['label','tweet'],header=0)\n    #train=data_csv.sample(frac=0.8,random_state=200) #random state is a seed value\n    #test=data_csv.drop(train.index)\n    #train.to_csv('train.csv')\n    #test.to_csv('test.csv')\n    return train, test\n\ntrain_path = \"../input/data-without-bad-words/train_blank.csv\"\ntest_path = \"../input/data-without-bad-words/test_blank.csv\"\ntrain_csv, test_csv = read_train_and_test(train_path, test_path)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_csv.tweet.values[:10])","execution_count":5,"outputs":[{"output_type":"stream","text":"['# anywere & # 8220 ; @ TheCooleyShow : LA = palm trees and great weather Any were else = horrible weather and no   & # 8221 ;'\n 'RT @ OfficialA1King : The face you make when you see a   trying to preach on twitter http : //t.co/2T1UkUDQBw'\n '  get off my twitter   & # 128074 ;'\n 'I can taste loud n   on my   & # 128541 ;'\n 'Diabetes galore & # 128514 ; & # 128514 ; & # 128514 ; & # 128514 ; & # 128514 ; RT @ TIME : Colorado health officials recommend   brownie ban http : //t.co/Z59oy20TMp'\n '@ LaneBelgarde @ A7XDemery you     didnt even invite me'\n 'RT @ djMemphis10 : Never hurt your Girlfriend to make a   happy & # 128175 ; # djmemphis10'\n 'This is why nothing gets done .   like Coburn trying 2 ad a   amendment 2 a public works bill . Push back on the # teabagger # morningjoe'\n 'RT @ ccancel14 : Miserable   talk the most   & # 128514 ; & # 9996 ; & # 65039 ;'\n 'Its gorg out ! RT @ @ anggxo : This   thinks its june or something @ BriiiXO http : //t.co/mFVCmym']\n","name":"stdout"}]},{"metadata":{"executionInfo":{"elapsed":9710,"status":"ok","timestamp":1615028091307,"user":{"displayName":"Schlözer","photoUrl":"","userId":"15113573867308042460"},"user_tz":-60},"id":"DvIQKI-AtzJY","outputId":"177a498e-b1d1-4bc0-d9bf-ce4bab3b0183","trusted":true},"cell_type":"code","source":"def run_tokenizer(train_csv, test_csv, merge_label=False):\n    tokenizer = DistilBertTokenizerFast.from_pretrained('bert-base-uncased') \n\n    def get_max_len(tokenizer, train_csv):\n        tweets = train_csv.tweet.values\n        max_length = 0\n        for t in tweets:\n          ids = tokenizer.encode(t)\n          max_length = max(len(ids),max_length)\n        return max_length\n\n    max_length = get_max_len(tokenizer, train_csv)\n    train_tweets, train_labels = train_csv.tweet.values, train_csv.label.values\n    test_tweets, test_labels = test_csv.tweet.values, test_csv.label.values\n    if merge_label == True:\n       train_labels = [l if l ==0 else 1 for l in train_labels]\n       test_labels = [l if l ==0 else 1 for l in test_labels]\n\n    def tokenize_for_tweet(tokenizer, tweets, labels):\n        input_ids = []\n        attention_masks = []\n\n        for t in tweets:\n            input_dict = tokenizer.encode_plus(t, add_special_tokens=True, max_length=max_length, truncation=True, padding='max_length',return_tensors='pt')\n            input_ids.append(input_dict['input_ids'])\n            attention_masks.append(input_dict['attention_mask'])\n        input_ids = torch.cat(input_ids,dim=0)\n        attention_masks = torch.cat(attention_masks,dim=0)\n        labels=torch.tensor(labels)\n        dataset = TensorDataset(input_ids, attention_masks, labels)\n        return dataset\n        \n    train_dataset = tokenize_for_tweet(tokenizer, train_tweets, train_labels)\n    test_dataset = tokenize_for_tweet(tokenizer, test_tweets, test_labels)\n    num_label = 3 if merge_label == False else 2\n    return train_dataset, test_dataset, num_label\n\ntrain_dataset, test_dataset, num_label = run_tokenizer(train_csv, test_csv, merge_label = True)","execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52695258f9994827932b48e0041fd617"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb8ae462db684837882ee376fa51e34a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c794acdfacd4d98bb9a7e6efb66cf0a"}},"metadata":{}}]},{"metadata":{"executionInfo":{"elapsed":625,"status":"ok","timestamp":1615028093993,"user":{"displayName":"Schlözer","photoUrl":"","userId":"15113573867308042460"},"user_tz":-60},"id":"mAfIYownmjsZ","trusted":true},"cell_type":"code","source":"batch_size = 10\n\ntrain_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset), batch_size = batch_size)\n\ntest_dataloader = DataLoader(test_dataset, sampler = SequentialSampler(test_dataset), batch_size = batch_size)","execution_count":7,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":19844,"status":"ok","timestamp":1615028116321,"user":{"displayName":"Schlözer","photoUrl":"","userId":"15113573867308042460"},"user_tz":-60},"id":"BqRtXoCtnXwS","outputId":"fe8bd7de-a9b9-4415-fb5e-3860cb96845d","trusted":true},"cell_type":"code","source":"bert_model = BertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels = num_label)\nbert_model = bert_model.to(DEVICE)","execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/442 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17be2a3571f04e508f62b91d6a926d4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dab1b3982f7a434db66d49cd91a4a46e"}},"metadata":{}},{"output_type":"stream","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertForSequenceClassification: ['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","name":"stderr"}]},{"metadata":{"executionInfo":{"elapsed":13108,"status":"ok","timestamp":1615028116323,"user":{"displayName":"Schlözer","photoUrl":"","userId":"15113573867308042460"},"user_tz":-60},"id":"5FAxcsnLHYWp","trusted":true},"cell_type":"code","source":"def evaluate(model, best_acc, test_dataloader):\n    num_total, num_correct = 0, 0\n    model.train(False)\n    with torch.no_grad():\n      eval_loss = 0\n\n      for input, mask, label in test_dataloader:\n          model.zero_grad()\n          \n          input = input.to(DEVICE)\n          mask = mask.to(DEVICE)\n          label = label.to(DEVICE)\n\n          loss, output = model(input_ids=input, attention_mask=mask, labels=label, return_dict=False)\n\n          predict_label = torch.argmax(output, dim=1)\n\n          num_correct += (predict_label == label).sum().item()\n          num_total += len(label)\n\n          eval_loss += loss.item()\n\n      acc = num_correct/num_total\n      if acc > best_acc:\n        best_acc = acc\n        torch.save(model, 'model'+str(best_acc))\n\n    print('eval_loss: {}, accuracy: {}'.format(eval_loss,acc))\n    return best_acc\n","execution_count":9,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":514,"status":"ok","timestamp":1615028128806,"user":{"displayName":"Schlözer","photoUrl":"","userId":"15113573867308042460"},"user_tz":-60},"id":"laM3Y-4qFMRN","trusted":true},"cell_type":"code","source":"def training(model, epochs, optimizer, train_dataloader, test_dataloader, scheduler=None):\n    best_acc = 0\n    for e in range(epochs):\n        print('training {} epoch...'.format(e+1))\n        start_time = time.time()\n\n        train_loss =0\n\n        model.train(True)\n        for input, mask, label in train_dataloader:\n            input = input.to(DEVICE)\n            mask = mask.to(DEVICE)\n            label=label.to(DEVICE)\n            \n            model.zero_grad()\n\n            loss, output = model(input_ids=input, attention_mask=mask, labels=label,return_dict=False)\n\n            train_loss += loss.item()\n            loss.backward()\n\n            #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n    \n            optimizer.step()\n\n            if scheduler:\n              scheduler.step()\n\n        sec = time.time()-start_time\n        print('{} seconds used......'.format(sec))\n        print(\"{} training finished! train loss: {}\".format(e+1,train_loss))\n        print('evaluating...')\n        best_acc = evaluate(model, best_acc, test_dataloader)","execution_count":10,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":715,"status":"ok","timestamp":1615028141506,"user":{"displayName":"Schlözer","photoUrl":"","userId":"15113573867308042460"},"user_tz":-60},"id":"YCK8e9HloLJr","trusted":true},"cell_type":"code","source":"optimizer = AdamW(bert_model.parameters(),lr = 2e-5, eps = 1e-8)","execution_count":11,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":637,"status":"ok","timestamp":1615028143143,"user":{"displayName":"Schlözer","photoUrl":"","userId":"15113573867308042460"},"user_tz":-60},"id":"jQcp_8CiqdtD","trusted":true},"cell_type":"code","source":"epochs = 10\n\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)","execution_count":12,"outputs":[]},{"metadata":{"id":"SoXzLzmgKP8Z","outputId":"249c849f-fd00-4306-9e3b-4a26ad9e65cd","trusted":true},"cell_type":"code","source":"training(bert_model, epochs, optimizer, train_dataloader, test_dataloader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predicting_labels(model, test_dataloader):\n    true_labels, predict_labels = [], []\n    for input, mask, label in test_dataloader:\n        model.zero_grad()\n        input=input.to(DEVICE)\n        mask=mask.to(DEVICE)\n        label=label.to(DEVICE)\n        loss, logits = model(input_ids=input, attention_mask=mask, labels=label, return_dict=False)\n        predict_label = torch.argmax(logits, dim=1)\n        true_labels+=(label.tolist())\n        predict_labels+=(predict_label.tolist())\n    return true_labels, predict_labels","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = torch.load('../input/remove-swear-with-blank-model/model0.903772443009885')\ntrue_labels, predict_labels = predicting_labels(model, test_dataloader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy=accuracy_score(true_labels,predict_labels)\nprint(accuracy)\npre, re, f1, sup = precision_recall_fscore_support(true_labels,predict_labels)\nprint(pre,re,f1,sup)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import classification_report\nimport numpy as np\nimport random","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(y_true, y_pred, classes, normalize=False, title=None, cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    #classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"cm = confusion_matrix(true_labels, predict_labels)\nplot_confusion_matrix(true_labels, predict_labels, classes=['hate','no-hate'] ,title='Confusion matrix without rebalance')\n\n#Plot normalized confusion matrix\nplot_confusion_matrix(true_labels, predict_labels, classes=['hate','no-hate'], normalize=True,title='Normalized confusion matrix without rebalance')\n\nplt.show()\n\nprint(classification_report(true_labels, predict_labels, target_names=['hate','no-hate']))","execution_count":16,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'true_labels' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-4e92a864b885>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hate'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'no-hate'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Confusion matrix without rebalance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Plot normalized confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hate'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'no-hate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Normalized confusion matrix without rebalance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'true_labels' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rebalance_dataset(dataset):\n    dataset = sorted(dataset, key=lambda x:x[2])\n    num_label_1 = len([inst for inst in dataset if inst[2]==1])\n    num_label_0 = len([inst for inst in dataset if inst[2]==0]) \n    balanced_dataset = dataset[:num_label_0] + dataset[num_label_1:num_label_0 + num_label_1]\n    random.shuffle(balanced_dataset)\n    return balanced_dataset","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"re_train_dataset = rebalance_dataset(train_dataset)\nre_test_dataset = rebalance_dataset(test_dataset)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"re_train_dataloader = DataLoader(re_train_dataset, sampler = RandomSampler(re_train_dataset), batch_size = batch_size)\nre_test_dataloader = DataLoader(test_dataset, sampler = RandomSampler(test_dataset), batch_size = 128)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training(bert_model, epochs, optimizer, re_train_dataloader, re_test_dataloader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = torch.load('../input/remove-swear-with-blank-model/model0.903772443009885')\ntrue_labels2, predict_labels2 = predicting_labels(model2, re_test_dataloader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy2=accuracy_score(true_labels2,predict_labels2)\nprint(accuracy2)\npre2, re2, f1_2, sup2 = precision_recall_fscore_support(true_labels2,predict_labels2)\nprint(pre2,re2,f1_2,sup2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(true_labels2, predict_labels2)\nplot_confusion_matrix(true_labels2, predict_labels2, classes=['hate','no-hate'] ,title='Confusion matrix with rebalance')\n\n#Plot normalized confusion matrix\nplot_confusion_matrix(true_labels2, predict_labels2, classes=['hate','no-hate'], normalize=True,title='Normalized confusion matrix with rebalance')\n\nplt.show()\n\nprint(classification_report(true_labels2, predict_labels2, target_names=['hate','no-hate']))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}